{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMvDmRudwrPsGl6TqRM69g5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lalithaaindrapramad/11239A015-NLP_OBSERVATION/blob/main/11239A015_NLP_OBSERVATION.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mnYHBNMLFn3g",
        "outputId": "b9782382-e127-4b4d-b945-a39a48604b35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Tokens:\n",
            "['Hello', '!', 'This', 'is', 'a', 'simple', 'NLTK', 'tokenization', 'example', '.']\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download tokenizer models (run once)\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab') # Added to download the missing resource\n",
        "\n",
        "text = \"Hello! This is a simple NLTK tokenization example.\"\n",
        "\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "print(\"Word Tokens:\")\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download resources (run once)\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "text = \"The striped bats are hanging on their feet for best\"\n",
        "\n",
        "# Tokenize\n",
        "words = word_tokenize(text)\n",
        "\n",
        "# Initialize lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Lemmatize each word\n",
        "lemmas = [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "print(\"Original Words:\")\n",
        "print(words)\n",
        "print(\"\\nLemmatized Words:\")\n",
        "print(lemmas)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xRcQwoT7HjU8",
        "outputId": "e577689f-db01-4337-debb-c0a6d567834c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Words:\n",
            "['The', 'striped', 'bats', 'are', 'hanging', 'on', 'their', 'feet', 'for', 'best']\n",
            "\n",
            "Lemmatized Words:\n",
            "['The', 'striped', 'bat', 'are', 'hanging', 'on', 'their', 'foot', 'for', 'best']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "\n",
        "# Download tokenizer (run once)\n",
        "nltk.download('punkt')\n",
        "\n",
        "text = \"I am enjoying the challenges of working and learning.\"\n",
        "\n",
        "# Tokenize\n",
        "words = word_tokenize(text)\n",
        "\n",
        "# Initialize stemmer\n",
        "ps = PorterStemmer()\n",
        "\n",
        "# Stem each word\n",
        "stems = [ps.stem(word) for word in words]\n",
        "\n",
        "print(\"Original Words:\")\n",
        "print(words)\n",
        "print(\"\\nStemmed Words:\")\n",
        "print(stems)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nG-yZj18HnRi",
        "outputId": "b14959b0-8790-4a49-8989-7f75ccd95cf0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Words:\n",
            "['I', 'am', 'enjoying', 'the', 'challenges', 'of', 'working', 'and', 'learning', '.']\n",
            "\n",
            "Stemmed Words:\n",
            "['i', 'am', 'enjoy', 'the', 'challeng', 'of', 'work', 'and', 'learn', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download resources (run once)\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "text = \"The children were playing happily in the gardens.\"\n",
        "\n",
        "# Tokenize words\n",
        "words = word_tokenize(text)\n",
        "\n",
        "# Initialize stemmer and lemmatizer\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "print(\"Word | Lemma | Stem\")\n",
        "print(\"-----------------------------\")\n",
        "for word in words:\n",
        "    lemma = lemmatizer.lemmatize(word.lower())   # default noun lemmatization\n",
        "    stem = stemmer.stem(word)\n",
        "    print(f\"{word:12} {lemma:10} {stem}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zuvRr7bBITrl",
        "outputId": "30de0171-57a2-4f92-ec4f-b57986e9cbff"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word | Lemma | Stem\n",
            "-----------------------------\n",
            "The          the        the\n",
            "children     child      children\n",
            "were         were       were\n",
            "playing      playing    play\n",
            "happily      happily    happili\n",
            "in           in         in\n",
            "the          the        the\n",
            "gardens      garden     garden\n",
            ".            .          .\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "text = \"I havv a speling errror in this sentense.\"\n",
        "\n",
        "blob = TextBlob(text)\n",
        "\n",
        "corrected = blob.correct()\n",
        "\n",
        "print(\"Original:\", text)\n",
        "print(\"Corrected:\", corrected)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Yijokm7IoAR",
        "outputId": "f302a02c-c088-4c1b-97f3-0ee9f438b0c1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original: I havv a speling errror in this sentense.\n",
            "Corrected: I have a spelling error in this sentence.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.sem import Expression\n",
        "from nltk.inference import ResolutionProver\n",
        "\n",
        "nltk.download('wordnet')  # optional if using WordNet\n",
        "\n",
        "# Enable logic parsing\n",
        "read_expr = Expression.fromstring\n",
        "\n",
        "# Knowledge base: some facts and rules\n",
        "# Example: All men are mortal, Socrates is a man\n",
        "kb = [\n",
        "    read_expr('all x. (man(x) -> mortal(x))'),  # rule: men are mortal\n",
        "    read_expr('man(socrates)')                  # fact: Socrates is a man\n",
        "]\n",
        "\n",
        "# Query: Is Socrates mortal?\n",
        "query = read_expr('mortal(socrates)')\n",
        "\n",
        "# Use resolution prover for deduction\n",
        "proved = ResolutionProver().prove(query, kb)\n",
        "\n",
        "print(f\"Is Socrates mortal? -> {proved}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "683ZwmKhL2Jx",
        "outputId": "72b89395-f7c9-4d1f-9f4d-c88f292ce4e6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Is Socrates mortal? -> True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.util import ngrams\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download tokenizer (run once)\n",
        "nltk.download('punkt')\n",
        "\n",
        "text = \"Natural language processing with NLTK is fun.\"\n",
        "\n",
        "# Tokenize\n",
        "words = word_tokenize(text)\n",
        "\n",
        "# Generate bigrams (n = 2)\n",
        "bigrams = list(ngrams(words, 2))\n",
        "\n",
        "# Generate trigrams (n = 3)\n",
        "trigrams = list(ngrams(words, 3))\n",
        "\n",
        "# Generate n-grams (example: 4-grams)\n",
        "fourgrams = list(ngrams(words, 4))\n",
        "\n",
        "print(\"Words:\", words)\n",
        "print(\"\\nBigrams:\", bigrams)\n",
        "print(\"\\nTrigrams:\", trigrams)\n",
        "print(\"\\nFourgrams:\", fourgrams)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v36ubc7VI_U-",
        "outputId": "a9ddea85-44f6-46b0-fdd0-73c483be510c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Words: ['Natural', 'language', 'processing', 'with', 'NLTK', 'is', 'fun', '.']\n",
            "\n",
            "Bigrams: [('Natural', 'language'), ('language', 'processing'), ('processing', 'with'), ('with', 'NLTK'), ('NLTK', 'is'), ('is', 'fun'), ('fun', '.')]\n",
            "\n",
            "Trigrams: [('Natural', 'language', 'processing'), ('language', 'processing', 'with'), ('processing', 'with', 'NLTK'), ('with', 'NLTK', 'is'), ('NLTK', 'is', 'fun'), ('is', 'fun', '.')]\n",
            "\n",
            "Fourgrams: [('Natural', 'language', 'processing', 'with'), ('language', 'processing', 'with', 'NLTK'), ('processing', 'with', 'NLTK', 'is'), ('with', 'NLTK', 'is', 'fun'), ('NLTK', 'is', 'fun', '.')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.util import ngrams\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "# Download tokenizer (run once)\n",
        "nltk.download('punkt')\n",
        "\n",
        "text = \"I love natural language processing and I love learning NLP\"\n",
        "\n",
        "# Tokenize and add sentence boundary tokens\n",
        "words = [\"<s>\"] + word_tokenize(text.lower()) + [\"</s>\"]\n",
        "\n",
        "# Unigrams and bigrams\n",
        "unigrams = list(ngrams(words, 1))\n",
        "bigrams = list(ngrams(words, 2))\n",
        "\n",
        "# Count frequencies\n",
        "unigram_freq = Counter(unigrams)\n",
        "bigram_freq = Counter(bigrams)\n",
        "\n",
        "# Vocabulary size\n",
        "V = len(unigram_freq)\n",
        "\n",
        "# Function for Laplace-smoothed bigram probability\n",
        "def laplace_bigram_prob(w1, w2):\n",
        "    bigram = (w1, w2)\n",
        "    unigram = (w1,)\n",
        "\n",
        "    # Laplace smoothing\n",
        "    prob = (bigram_freq[bigram] + 1) / (unigram_freq[unigram] + V)\n",
        "    return prob\n",
        "\n",
        "# Test words\n",
        "print(\"Laplace Smoothed Bigram Probabilities:\\n\")\n",
        "test_bigrams = [(\"i\", \"love\"), (\"love\", \"learning\"), (\"natural\", \"language\")]\n",
        "\n",
        "for w1, w2 in test_bigrams:\n",
        "    print(f\"P({w2} | {w1}) = {laplace_bigram_prob(w1, w2):.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVYwDX4oJRV7",
        "outputId": "e128fbcd-899e-48e6-b91b-76cca4800132"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Laplace Smoothed Bigram Probabilities:\n",
            "\n",
            "P(love | i) = 0.2500\n",
            "P(learning | love) = 0.1667\n",
            "P(language | natural) = 0.1818\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download required resources (run once)\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger_eng') # Changed to download specific English tagger\n",
        "\n",
        "text = \"Natural language processing with NLTK is fun and educational.\"\n",
        "\n",
        "# Tokenize the text\n",
        "words = word_tokenize(text)\n",
        "\n",
        "# POS tagging\n",
        "pos_tags = nltk.pos_tag(words)\n",
        "\n",
        "print(\"Word | POS Tag\")\n",
        "print(\"---------------------\")\n",
        "for word, tag in pos_tags:\n",
        "    print(f\"{word:15} {tag}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6BlBBgyJq-q",
        "outputId": "a4256c10-7a0f-4223-c6d2-9cf4b7c4cd0b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word | POS Tag\n",
            "---------------------\n",
            "Natural         JJ\n",
            "language        NN\n",
            "processing      NN\n",
            "with            IN\n",
            "NLTK            NNP\n",
            "is              VBZ\n",
            "fun             NN\n",
            "and             CC\n",
            "educational     JJ\n",
            ".               .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import treebank\n",
        "\n",
        "# Download required NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('treebank')\n",
        "\n",
        "# Prepare training data: list of POS-tagged sentences\n",
        "train_sents = treebank.tagged_sents()[:3000]  # first 3000 sentences for training\n",
        "test_sents = treebank.tagged_sents()[3000:]   # rest for testing\n",
        "\n",
        "# Train Unigram and Bigram taggers\n",
        "unigram_tagger = nltk.UnigramTagger(train_sents)\n",
        "bigram_tagger = nltk.BigramTagger(train_sents, backoff=unigram_tagger)\n",
        "\n",
        "# Test sentence\n",
        "sentence = \"The quick brown fox jumps over the lazy dog\".split()\n",
        "\n",
        "# POS tagging using the bigram tagger\n",
        "pos_tags = bigram_tagger.tag(sentence)\n",
        "\n",
        "print(\"Word | POS Tag\")\n",
        "print(\"---------------------\")\n",
        "for word, tag in pos_tags:\n",
        "    print(f\"{word:10} {tag}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RlsTKwkjJzNY",
        "outputId": "252af6ce-b4ed-4fbb-e493-63431697f691"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/treebank.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word | POS Tag\n",
            "---------------------\n",
            "The        DT\n",
            "quick      JJ\n",
            "brown      None\n",
            "fox        None\n",
            "jumps      None\n",
            "over       IN\n",
            "the        DT\n",
            "lazy       None\n",
            "dog        None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tag import hmm\n",
        "from nltk.corpus import treebank\n",
        "\n",
        "# Download required NLTK resources\n",
        "nltk.download('treebank')\n",
        "nltk.download('universal_tagset')\n",
        "\n",
        "# Prepare training and testing data\n",
        "tagged_sents = treebank.tagged_sents(tagset='universal')\n",
        "train_sents = tagged_sents[:3000]\n",
        "test_sents = tagged_sents[3000:]\n",
        "\n",
        "# Train an HMM tagger\n",
        "trainer = hmm.HiddenMarkovModelTrainer()\n",
        "hmm_tagger = trainer.train_supervised(train_sents)\n",
        "\n",
        "# Test sentence\n",
        "sentence = \"The quick brown fox jumps over the lazy dog\".split()\n",
        "\n",
        "# Tag the sentence using the HMM tagger\n",
        "tags = hmm_tagger.tag(sentence)\n",
        "\n",
        "print(\"Word | POS Tag\")\n",
        "print(\"-------------------\")\n",
        "for word, tag in tags:\n",
        "    print(f\"{word:10} {tag}\")\n",
        "\n",
        "# Evaluate accuracy on test data\n",
        "accuracy = hmm_tagger.evaluate(test_sents)\n",
        "print(f\"\\nHMM Tagger Accuracy on test data: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHNOPrGZKuhC",
        "outputId": "410051ac-ada1-47e5-b529-0fd126531938"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Package treebank is already up-to-date!\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n",
            "/usr/local/lib/python3.12/dist-packages/nltk/tag/hmm.py:335: RuntimeWarning: overflow encountered in cast\n",
            "  O[i, k] = self._output_logprob(si, self._symbols[k])\n",
            "/usr/local/lib/python3.12/dist-packages/nltk/tag/hmm.py:363: RuntimeWarning: overflow encountered in cast\n",
            "  O[i, k] = self._output_logprob(si, self._symbols[k])\n",
            "/tmp/ipython-input-3777859507.py:30: DeprecationWarning: \n",
            "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
            "  instead.\n",
            "  accuracy = hmm_tagger.evaluate(test_sents)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word | POS Tag\n",
            "-------------------\n",
            "The        DET\n",
            "quick      ADJ\n",
            "brown      NOUN\n",
            "fox        NOUN\n",
            "jumps      NOUN\n",
            "over       NOUN\n",
            "the        NOUN\n",
            "lazy       NOUN\n",
            "dog        NOUN\n",
            "\n",
            "HMM Tagger Accuracy on test data: 0.5160\n"
          ]
        }
      ]
    }
  ]
}